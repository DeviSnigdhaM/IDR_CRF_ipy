{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image to Information\n",
    "#### Extracting Named Entities using Conditional Random Fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "We will be using a publicly available dataset consisting of receipts. Open a file in the below folders to look at the structure of the OCR outputs and annotations for the dataset we're using for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "allOCRSet = \"datasets/CleanOCR\" # A folder with all OCR files\n",
    "annotationSet = \"datasets/Annotations\" # A folder with all Annotation files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to OCR\n",
    "Use Pytesseract to apply OCR on a sample receipt image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Split Yerba Buena\n",
      "\n",
      "145 4th Street\n",
      "San Francisco, CA 94103\n",
      "Phone 415-296-8009\n",
      "\n",
      "10/22/2018 1:35:32 PM\n",
      "Order Id: AAAALGS4ACDE\n",
      "33 - +++DINE HERE+++\n",
      "\n",
      "2 VEGGIE BURGER (@10.95) $21.90\n",
      "1 SONOMA $11.95\n",
      "2 Sweet Potato Herb & Parmesan FREN $11.90\n",
      "_ 2 Fresh Squeezed Lemonade (@2.95) $5.90\n",
      "\n",
      "SF Employer Mandates $2.32\n",
      "Sub Total $53.97\n",
      "Sales Tax $4.59\n",
      "Order Total $58 .56\n",
      "American Express $58 .56\n",
      "\n",
      "Card#: RRR 1000\n",
      "Authorization: 573076\n",
      "\n",
      "--> Order Closed <--\n",
      "\n",
      "Order online at splitbread.com\n",
      "\n",
      " \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "image_path = \"datasets/SplitYB.png\" # Receipt image from a restaurant\n",
    "\n",
    "def getOCRText(image_path):\n",
    "    image_data = Image.open(image_path)\n",
    "    image_text = pytesseract.image_to_string(image_data)\n",
    "    return image_text\n",
    "\n",
    "print(getOCRText(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Labeling\n",
    "Tokenize, Annotate and Tag a receipt for the total amount and merchant/business name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerDataset = \"datasets/ner-dataset.csv\"\n",
    "def dataAnnotation():\n",
    "\tallCleanOCRFiles = os.listdir(allOCRSet) # Load all Clean OCR Files\n",
    "\tallIXFiles = os.listdir(annotationSet) # Load all Clean OCR Files\n",
    "\toutfile = open(nerDataset, 'w')\n",
    "\treceiptNum = 1\n",
    "\toutfile.write('Receipt #,Word,POS,Tag' + '\\n')\n",
    "\n",
    "\tfor ixfile in allIXFiles:\n",
    "\t\tfilename = os.fsdecode(ixfile)\n",
    "\t\tfor ocrfile in allCleanOCRFiles:\n",
    "\t\t\tocrFilename = os.fsdecode(ocrfile)\n",
    "\t\t\tif filename == ocrFilename:\n",
    "\t\t\t\tixData = {}\n",
    "\t\t\t\tocrCleanLine = \"\"\n",
    "\t\t\t\twith open(annotationSet + '/' + filename) as f1:\n",
    "\t\t\t\t\tixData = json.load(f1)\n",
    "\t\t\t\twith open(allOCRSet + '/' + filename) as f2:\n",
    "\t\t\t\t\tocrCleanLine = f2.readlines()\n",
    "\t\t\t\tannotationLine = \"\"\n",
    "\t\t\t\twordNum = 0\n",
    "\t\t\t\twords = ocrCleanLine[0].replace(',', '').split(' ')\n",
    "\t\t\t\tisAmount = False\n",
    "\t\t\t\tisDate = False\n",
    "\t\t\t\tisMerchant = False\n",
    "\t\t\t\tbeginMerchant = False\n",
    "\t\t\t\tbeginDate = False\n",
    "\n",
    "\t\t\t\tmerchantParts = ixData['company'].split(' ')\n",
    "\t\t\t\tdateParts = ixData['date'].split(' ')\n",
    "\t\t\t\t\n",
    "\t\t\t\tfor word in words:\n",
    "\t\t\t\t\tannotationLine = \"\"\n",
    "\t\t\t\t\tword = word.replace(',', ' ')\n",
    "\t\t\t\t\tif wordNum == 0:\n",
    "\t\t\t\t\t\tif word == dateParts[0] and not isDate:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"DT\" + ',' + 'B-date'\n",
    "\t\t\t\t\t\t\tisDate = True\n",
    "\t\t\t\t\t\t\t# Only expect a I-date if there are multiple parts to the date\n",
    "\t\t\t\t\t\t\tif len(dateParts) > 1:\n",
    "\t\t\t\t\t\t\t\tbeginDate = True\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tbeginDate = False\n",
    "\t\t\t\t\t\telif word == ixData['total'] and not isAmount:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"AM\" + ',' + 'B-amt'\n",
    "\t\t\t\t\t\t\tisAmount = True\n",
    "\t\t\t\t\t\telif word == merchantParts[0] and not isMerchant:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"MR\" + ',' + 'B-mer'\n",
    "\t\t\t\t\t\t\tisMerchant = True\n",
    "\t\t\t\t\t\t\tbeginMerchant = True\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"NN\" + ',' + 'O'\n",
    "\t\t\t\t\telse:                        \n",
    "\t\t\t\t\t\t# Amount tagging\n",
    "\t\t\t\t\t\tif word == ixData['total'] and not isAmount:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"AM\" + ',' + 'B-amt'\n",
    "\t\t\t\t\t\t\tisAmount = True\n",
    "\n",
    "\t\t\t\t\t\t# Merchant tagging\n",
    "\t\t\t\t\t\telif word == merchantParts[0] and not isMerchant:\n",
    "\t\t\t\t\t\t\tbeginMerchant = True\n",
    "\t\t\t\t\t\t\tisMerchant = True\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"MR\" + ',' + 'B-mer'\n",
    "\t\t\t\t\t\telif word in ixData['company'] and word != merchantParts[0] and word != merchantParts[-1] and beginMerchant:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"MR\" + ',' + 'I-mer'\n",
    "\t\t\t\t\t\telif word == merchantParts[-1] and beginMerchant:\n",
    "\t\t\t\t\t\t\tbeginMerchant = False\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"MR\" + ',' + 'I-mer'\n",
    "\n",
    "\t\t\t\t\t\t#Other tagging\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"NN\" + ',' + 'O'\n",
    "\t\t\t\t\twordNum += 1\t\t\t\t\t\t\n",
    "\t\t\t\t\t#print(annotationLine)\n",
    "\t\t\t\t\toutfile.write(annotationLine + '\\n')\n",
    "\t\t\t\treceiptNum += 1\n",
    "\toutfile.close\n",
    "    \n",
    "dataAnnotation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Model Training\n",
    "Train a model using Conditional Random Fields algorithm with Named Entity Recognition concept to extract amount and merchant fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_f1_score\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file\n",
    "df = pd.read_csv('datasets/ner-dataset.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "#Display first 10 rows\n",
    "df.head(10)\n",
    "df.describe()\n",
    "#Displaying the unique Tags\n",
    "df['Tag'].unique()\n",
    "#Checking null values, if any.\n",
    "df.isnull().sum()\n",
    "df = df.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                       s['POS'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Receipt #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_text(self):\n",
    "        try:\n",
    "            s = self.grouped['Receipt: {}'.format(self.n_sent)]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "#Displaying one full sentence\n",
    "getter = sentence(df)\n",
    "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
    "sentences[0]\n",
    "\n",
    "#sentence with its pos and tag.\n",
    "sent = getter.get_text()\n",
    "# print(sent)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele   \n",
    "    \n",
    "    # return string   \n",
    "    return str1\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=False, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.6)\n",
    "\n",
    "crf = CRF(algorithm = 'lbfgs',\n",
    "         c1 = 0.1,\n",
    "         c2 = 0.1,\n",
    "         max_iterations = 100,\n",
    "         all_possible_transitions = False)\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grand B-mer\n",
      "lux I-mer\n"
     ]
    }
   ],
   "source": [
    "#Predicting on the test set.\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "\tprediction = y_pred[i]\n",
    "\ttestList = X_test[i]\n",
    "\ttestSentence = \"\"\n",
    "\tfor testTuple in testList:\n",
    "\t\ttestSentence = testSentence + testTuple['word.lower()'] + ' '\n",
    "\twords = testSentence.split(\" \")\n",
    "\tx = 0\n",
    "\tfor wordPrediction in prediction:\n",
    "\t\tif wordPrediction == 'B-date' or wordPrediction == 'B-amt' or wordPrediction == 'B-mer' or wordPrediction == 'I-mer' or wordPrediction == 'I-date':\n",
    "\t\t\tprint(words[x],wordPrediction)\n",
    "\t\tx +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Report\n",
    "Print the results of how your model was trained and tested to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9639573493197691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-amt       0.00      0.00      0.00         2\n",
      "       B-mer       1.00      0.50      0.67         2\n",
      "       I-mer       1.00      0.33      0.50         3\n",
      "           O       0.97      1.00      0.99       177\n",
      "\n",
      "    accuracy                           0.97       184\n",
      "   macro avg       0.74      0.46      0.54       184\n",
      "weighted avg       0.96      0.97      0.96       184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "f1_score = flat_f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(f1_score)\n",
    "\n",
    "report = flat_classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application\n",
    "Go back above to Text Labeling section and add the below code under #Date Tagging section. Continue with the steps to generate a model that now extracts 3 fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\t\t\t\t\t# Date tagging\n",
    "\t\t\t\t\t\tif word == dateParts[0] and not isDate:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"DT\" + ',' + 'B-date'\n",
    "\t\t\t\t\t\t\tisDate = True\n",
    "\t\t\t\t\t\t\t# Only expect a I-date if there are multiple parts to the date\n",
    "\t\t\t\t\t\t\tif len(dateParts) > 1:\n",
    "\t\t\t\t\t\t\t\t# print(\"This date has many parts: \" + ixData['date'])\n",
    "\t\t\t\t\t\t\t\tbeginDate = True\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tbeginDate = False\n",
    "\t\t\t\t\t\telif word in ixData['date'] and word != dateParts[0] and word != dateParts[-1] and beginDate:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"DT\" + ',' + 'I-date'\n",
    "\t\t\t\t\t\telif word == dateParts[-1] and beginDate:\n",
    "\t\t\t\t\t\t\tbeginDate = False\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"DT\" + ',' + 'I-date'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test End to End by uploading a new image - OCR to NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bon Appetit\n",
      "Oracle Redwood\n",
      "Cafe 300\n",
      "\n",
      "ES\n",
      "rm CHK 2042\n",
      "6/14/2018 | 1.05 PM\n",
      "1! & g Salad, Large Ceasar 5 09\n",
      "1 Side, Chicken 2.39\n",
      "\n",
      " \n",
      "\n",
      "eeeERKHK( 03\n",
      "\n",
      "VISA:087861\n",
      "\n",
      "Credit Authorization 6,13\n",
      "VISA $8.13\n",
      "EKKKKKKKKKKD 4D\n",
      "\n",
      "Subtota | $7.48\n",
      "Tax $0.65\n",
      "Payment $8.13\n",
      "\n",
      "Change Due $O.00\n",
      "\n",
      "Seeerterene Check Closed -----------\n",
      "6/14/2018 1:05 PM\n",
      "\n",
      "   \n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "new_image_path = \"datasets/lunch.png\"\n",
    "test_file = getOCRText(new_image_path)\n",
    "print(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Bon Appetit\\nOracle Redwood\\nCafe 300\\n\\nES\\nrm CHK 2042\\n6/14/2018 | 1.05 PM\\n1! & g Salad, Large Ceasar 5 09\\n1 Side, Chicken 2.39\\n\\n \\n\\neeeERKHK( 03\\n\\nVISA:087861\\n\\nCredit Authorization 6,13\\nVISA $8.13\\nEKKKKKKKKKKD 4D\\n\\nSubtota | $7.48\\nTax $0.65\\nPayment $8.13\\n\\nChange Due $O.00\\n\\nSeeerterene Check Closed -----------\\n6/14/2018 1:05 PM\\n\\n   \\n\\x0c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-c5b1cab17f13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrial_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-c5b1cab17f13>\u001b[0m in \u001b[0;36mget_test_sentence\u001b[0;34m(test_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_test_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#Reading the csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Display first 10 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1991\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Bon Appetit\\nOracle Redwood\\nCafe 300\\n\\nES\\nrm CHK 2042\\n6/14/2018 | 1.05 PM\\n1! & g Salad, Large Ceasar 5 09\\n1 Side, Chicken 2.39\\n\\n \\n\\neeeERKHK( 03\\n\\nVISA:087861\\n\\nCredit Authorization 6,13\\nVISA $8.13\\nEKKKKKKKKKKD 4D\\n\\nSubtota | $7.48\\nTax $0.65\\nPayment $8.13\\n\\nChange Due $O.00\\n\\nSeeerterene Check Closed -----------\\n6/14/2018 1:05 PM\\n\\n   \\n\\x0c'"
     ]
    }
   ],
   "source": [
    "def get_test_sentence(test_file):\n",
    "    #Reading the csv file\n",
    "    trial = pd.read_csv(test_file, encoding = \"ISO-8859-1\")\n",
    "\n",
    "    #Display first 10 rows\n",
    "    trial.head(10)\n",
    "    trial.describe()\n",
    "    #Displaying the unique Tags\n",
    "    trial['Tag'].unique()\n",
    "    #Checking null values, if any.\n",
    "    trial.isnull().sum()\n",
    "    trial = trial.fillna(method = 'ffill')\n",
    "    trial_getter = sentence(trial) # Check last token - seems to be repeating \n",
    "    trial_sentences = [\" \".join([s[0] for s in sent]) for sent in trial_getter.sentences]\n",
    "    trial_sentences[0]\n",
    "\n",
    "    #sentence with its pos and tag.\n",
    "    trial_sent = trial_getter.get_text()\n",
    "    trial_sentences = trial_getter.sentences\n",
    "    return trial_sentences\n",
    "\n",
    "trial_sentences = get_test_sentence(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = [sent2features(s) for s in trial_sentences]\n",
    "ty = [sent2labels(s) for s in trial_sentences]\n",
    "\n",
    "#Predicting on the one test set.\n",
    "X_test = tX\n",
    "y_test = ty\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "f1_score = flat_f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(f1_score)\n",
    "\n",
    "report = flat_classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
