{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Image Is Worth a Thousand Words\n",
    "## Build Your First Intelligent Document Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New to Jupyter Notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a code cell\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tip: To run this cell type \"Shift + Enter\"\n",
    "print(\"Hello! Let's get started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition(NER) using Conditional Random Fields(CRF)\n",
    "\n",
    "NER is the method of extracting relevant information from a corpus of data and classifying those entities into predefined categories such as merchant, date, amount etc. Conditional Random fields are a type of discriminative machine learning classifiers that model the decision boundary and classify the OCR data into different output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Understanding the Dataset\n",
    "We will be using a publicly available dataset consisting of receipts. Open a file in the below folders to look at the structure of the OCR outputs and annotations for the dataset we're using for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see the receipt image \n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='datasets/1000-receipt.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Optical Character Recognition (OCR)\n",
    "Use Pytesseract to apply OCR on a sample receipt image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Receipt image from a restaurant\n",
    "image_path = \"datasets/1000-receipt.jpg\"\n",
    "\n",
    "# Run this to see the OCR text of the receipt\n",
    "def getOCRText(image_path):\n",
    "    image_data = Image.open(image_path)\n",
    "    image_text = pytesseract.image_to_string(image_data)\n",
    "    return image_text\n",
    "\n",
    "ocrText = getOCRText(image_path)\n",
    "print(ocrText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanOCRFiles(ocrText):\n",
    "    ocrlines = ocrText\n",
    "    cleanOCROutput = \"\"\n",
    "    for line in ocrlines.split('\\n'):\n",
    "        cleanOCROutput = cleanOCROutput + line.strip() + \" \"\n",
    "    return cleanOCROutput\n",
    "\n",
    "# Uncomment and Run this to see a clean OCR Text\n",
    "# cleanOcrText = cleanOCRFiles(ocrText)\n",
    "# print(cleanOcrText)\n",
    "\n",
    "allOCRSet = \"datasets/CleanOCR-all\" # A folder with all OCR files\n",
    "\n",
    "# Run this to see an example of Clean OCR text \n",
    "file = \"datasets/CleanOCR-all/1000-receipt.txt\"\n",
    "file_contents = open(file, 'r').read()\n",
    "print (file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Annotation\n",
    "Labeling of the truth data for the fields to be extracted. ex: date, amount and merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allAnnotationSet = \"datasets/Annotations-all\" # A folder with all Annotation files\n",
    "\n",
    "# Run this to see the OCR text \n",
    "file = \"datasets/Annotations-all/1000-receipt.txt\"\n",
    "\n",
    "file_contents = open(file, 'r').read()\n",
    "print (file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Text Labeling - Tokenize and Tag\n",
    "\n",
    "In order to get the text labeled and ready for the CRF model, we have to follow the following three steps:\n",
    "\n",
    "1. <p>Tokenize</p>\n",
    "Break the text into linguistic units. Ex: Tokenize on whitepspaces\n",
    "2. <p>Tag</p> \n",
    "    The BIO (short for Beginning, Inside, Outside) is a common tagging format for tagging tokens.\n",
    "    <ul>\n",
    "    <li>B- prefix before a tag indicates that the tag is the beginning of a chunk</li>\n",
    "    <li>I- prefix before a tag indicates that the tag is inside a chunk</li>\n",
    "    <li>O- tag indicates that a token belongs to no chunk (outside)</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allNerDataset = \"datasets/ner-dataset.csv\"\n",
    "\n",
    "def dataTagging(OCRset, annotationSet, nerDataset):\n",
    "\tallCleanOCRFiles = sorted(os.listdir(OCRset)) # Load all Clean OCR Files\n",
    "\tallIXFiles = sorted(os.listdir(annotationSet)) # Load all Annotated Truth Files\n",
    "\toutfile = open(nerDataset, 'w')\n",
    "\treceiptNum = 1\n",
    "\toutfile.write('Receipt #,Word,POS,Tag' + '\\n')\n",
    "\n",
    "\tfor ixfile in allIXFiles:\n",
    "\t\tfilename = os.fsdecode(ixfile)\n",
    "\t\tfor ocrfile in allCleanOCRFiles:\n",
    "\t\t\tocrFilename = os.fsdecode(ocrfile)\n",
    "\t\t\tif filename == ocrFilename and not filename.startswith('.'):\n",
    "\t\t\t\tixData = {}\n",
    "\t\t\t\tocrCleanLine = \"\"\n",
    "\t\t\t\twith open(annotationSet + '/' + filename) as f1:\n",
    "\t\t\t\t\tixData = json.load(f1)\n",
    "\t\t\t\twith open(OCRset + '/' + filename) as f2:\n",
    "\t\t\t\t\tocrCleanLine = f2.readlines()\n",
    "\t\t\t\tannotationLine = \"\"\n",
    "\t\t\t\twordNum = 0\n",
    "\t\t\t\twords = ocrCleanLine[0].replace(',', '').split(' ')\n",
    "\t\t\t\tisAmount = False\n",
    "\t\t\t\tisDate = False\n",
    "\t\t\t\tisMerchant = False\n",
    "\t\t\t\tbeginMerchant = False\n",
    "\t\t\t\tbeginDate = False\n",
    "\n",
    "\t\t\t\tmerchantParts = ixData['company'].split(' ')\n",
    "\t\t\t\tdateParts = ixData['date'].split(' ')\n",
    "\t\t\t\t\n",
    "\t\t\t\tfor word in words:\n",
    "\t\t\t\t\tannotationLine = \"\"\n",
    "\t\t\t\t\tword = word.replace(',', ' ')\n",
    "\t\t\t\t\tif wordNum == 0:\n",
    "\t\t\t\t\t\tif word == dateParts[0] and not isDate:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"DT\" + ',' + 'B-date'\n",
    "\t\t\t\t\t\t\tisDate = True\n",
    "\t\t\t\t\t\t\t# Only expect a I-date if there are multiple parts to the date\n",
    "\t\t\t\t\t\t\tif len(dateParts) > 1:\n",
    "\t\t\t\t\t\t\t\tbeginDate = True\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tbeginDate = False\n",
    "\t\t\t\t\t\telif word == ixData['total'] and not isAmount:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"AM\" + ',' + 'B-amt'\n",
    "\t\t\t\t\t\t\tisAmount = True\n",
    "\t\t\t\t\t\telif word == merchantParts[0] and not isMerchant:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"MR\" + ',' + 'B-mer'\n",
    "\t\t\t\t\t\t\tisMerchant = True\n",
    "\t\t\t\t\t\t\tbeginMerchant = True\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"NN\" + ',' + 'O'\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t#Date tagging\n",
    "\t\t\t\t\t\tif word == dateParts[0] and not isDate:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"DT\" + ',' + 'B-date'\n",
    "\t\t\t\t\t\t\tisDate = True\n",
    "\t\t\t\t\t\t\t# Only expect a I-date if there are multiple parts to the date\n",
    "\t\t\t\t\t\t\tif len(dateParts) > 1:\n",
    "\t\t\t\t\t\t\t\t# print(\"This date has many parts: \" + ixData['date'])\n",
    "\t\t\t\t\t\t\t\tbeginDate = True\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tbeginDate = False\n",
    "\t\t\t\t\t\telif word in ixData['date'] and word != dateParts[0] and word != dateParts[-1] and beginDate:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"DT\" + ',' + 'I-date'\n",
    "\t\t\t\t\t\telif word == dateParts[-1] and beginDate:\n",
    "\t\t\t\t\t\t\tbeginDate = False\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"DT\" + ',' + 'I-date'\n",
    "\t\t\t\t\t\t# Amount tagging\n",
    "\t\t\t\t\t\telif word == ixData['total'] and not isAmount:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"AM\" + ',' + 'B-amt'\n",
    "\t\t\t\t\t\t\tisAmount = True\n",
    "\n",
    "\t\t\t\t\t\t# Merchant tagging\n",
    "\t\t\t\t\t\telif word == merchantParts[0] and not isMerchant:\n",
    "\t\t\t\t\t\t\tbeginMerchant = True\n",
    "\t\t\t\t\t\t\tisMerchant = True\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"MR\" + ',' + 'B-mer'\n",
    "\t\t\t\t\t\telif word in ixData['company'] and word != merchantParts[0] and word != merchantParts[-1] and beginMerchant:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"MR\" + ',' + 'I-mer'\n",
    "\t\t\t\t\t\telif word == merchantParts[-1] and beginMerchant:\n",
    "\t\t\t\t\t\t\tbeginMerchant = False\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"MR\" + ',' + 'I-mer'\n",
    "\n",
    "\t\t\t\t\t\t#Other tagging\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"NN\" + ',' + 'O'\n",
    "\t\t\t\t\twordNum += 1\t\t\t\t\t\t\n",
    "\t\t\t\t\toutfile.write(annotationLine + '\\n')\n",
    "\t\t\t\treceiptNum += 1\n",
    "\toutfile.close\n",
    "\tprint ('Annotations for %d files completed!' %(int(receiptNum) - 1))\n",
    "\n",
    "dataTagging(allOCRSet, allAnnotationSet, allNerDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see the data annotations\n",
    "file = \"datasets/ner-dataset.csv\"\n",
    "\n",
    "file_contents = open(file, 'r').read()\n",
    "print (file_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: NER Model Training\n",
    "\n",
    "CRF is used for predicting sequences using contextual information. The CRF model is a supervised model that requires labelled data. The data required for our model training consists of tokens from a particular receipt with their corresponding Part of Speech (POS) tag and their BIO scheme tag. \n",
    "\n",
    "Below is the formula for CRF where y is the output variable and X is input sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"codeEqn.png\" alt=\"Equation\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_f1_score\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file generated with a 1000 files in the dataset\n",
    "df = pd.read_csv('datasets/ner-dataset-1000.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "#Checking null values, if any.\n",
    "df.isnull().sum()\n",
    "df = df.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                       s['POS'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Receipt #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_text(self):\n",
    "        try:\n",
    "            s = self.grouped['Receipt: {}'.format(self.n_sent)]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "getter = sentence(df)\n",
    "# Sentence with its pos and tag.\n",
    "sent = getter.get_text()\n",
    "# print(sent)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4a: Feature Functions\n",
    "\n",
    "We will be using the default features used by the NER in Natural Language Toolkit (NLTK) as a starting point. This includes word identity, word suffix, word shape and word POS tag in addition to some information from nearby words. We can modify and add to these features in order to improve accuracy as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4b: Train\n",
    "\n",
    "The data is then fed into the CRF model trainer provided by Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "# Splitting the data as 70:30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# 'lbfgs' - Gradient descent using the L-BFGS method\n",
    "# 'c1' - coefficient of L1 regularization\n",
    "# 'c2' - coefficient of L2 regularization\n",
    "# 'max_iterations' - max iterations for optimization algorithms\n",
    "# 'all_possible_transitions' - Specify whether CRFsuite generates transition features that do not even occur in the training data (i.e., negative transition features)\n",
    "crf = CRF(algorithm = 'lbfgs',\n",
    "         c1 = 0.1,\n",
    "         c2 = 0.1,\n",
    "         max_iterations = 100,\n",
    "         all_possible_transitions = False)\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4c: Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on the test set.\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "def printPredictions(y_pred):\n",
    "\tfor i in range(len(y_pred)):\n",
    "\t\tprediction = y_pred[i]\n",
    "\t\ttestList = X_test[i]\n",
    "\t\ttestSentence = \"\"\n",
    "\t\tfor testTuple in testList:\n",
    "\t\t\ttestSentence = testSentence + testTuple['word.lower()'] + ' '\n",
    "\t\twords = testSentence.split(\" \")\n",
    "\t\tx = 0\n",
    "\t\tfor wordPrediction in prediction:\n",
    "\t\t\tif wordPrediction == 'B-date' or wordPrediction == 'B-amt' or wordPrediction == 'B-mer' or wordPrediction == 'I-mer' or wordPrediction == 'I-date':\n",
    "\t\t\t\tprint('%s : %s' % (wordPrediction, words[x]))\n",
    "\t\t\tx +=1\n",
    "\n",
    "printPredictions(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4d: Generate Report and Analyze Results\n",
    "Print the results of how your model was trained and tested to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = flat_f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"Flat F1 Score: %f \\n\" %f1_score)\n",
    "\n",
    "report = flat_classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Try your lunch receipt\n",
    "\n",
    "Now, lets test a new receipt that the machine learning model hasn't encountered before.\n",
    "- Option 1: Use the lunch.png image available.\n",
    "- Option 2: Upload a receipt image to the dataset directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test End to End by uploading a new image - OCR to NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see the receipt image \n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='datasets/lunch.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "new_image_path = \"datasets/lunch.png\"\n",
    "test_file = getOCRText(new_image_path)\n",
    "\n",
    "test_ocr_sentence = test_file.replace(\"\\n\", \" \")\n",
    "print(test_ocr_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate and Use Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the OCR output from above to the OCR file in CleanOCR-test directory\n",
    "# Add the corresponding annotations to a file in the Annotations-test directory\n",
    "# Ensure that the 2 files have identical names in order for the next step to work as expected\n",
    "testOCRSet = \"datasets/CleanOCR-test\" # A folder with all OCR files\n",
    "testAnnotationSet = \"datasets/Annotations-test\" # A folder with all Annotation files\n",
    "testNerDataset = \"datasets/ner-dataset-test.csv\"\n",
    "\n",
    "dataTagging(testOCRSet,testAnnotationSet, testNerDataset)\n",
    "\n",
    "def get_test_sentence(test_file):\n",
    "    #Reading the csv file\n",
    "    trial = pd.read_csv(test_file, encoding = \"ISO-8859-1\")\n",
    "\n",
    "    #Display first 10 rows\n",
    "    trial.head(10)\n",
    "    trial.describe()\n",
    "    #Displaying the unique Tags\n",
    "    trial['Tag'].unique()\n",
    "    #Checking null values, if any.\n",
    "    trial.isnull().sum()\n",
    "    trial = trial.fillna(method = 'ffill')\n",
    "    trial_getter = sentence(trial) # Check last token - seems to be repeating \n",
    "    trial_sentences = [\" \".join([s[0] for s in sent]) for sent in trial_getter.sentences]\n",
    "    trial_sentences[0]\n",
    "\n",
    "    #sentence with its pos and tag.\n",
    "    trial_sent = trial_getter.get_text()\n",
    "    trial_sentences = trial_getter.sentences\n",
    "    return trial_sentences\n",
    "\n",
    "trial_sentences = get_test_sentence(testNerDataset)\n",
    "\n",
    "tX = [sent2features(s) for s in trial_sentences]\n",
    "ty = [sent2labels(s) for s in trial_sentences]\n",
    "\n",
    "#Predicting on the one test set.\n",
    "X_test = tX\n",
    "y_test = ty\n",
    "y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = flat_f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"Flat F1 Score: %f \\n\" %f1_score)\n",
    "\n",
    "report = flat_classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "printPredictions(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
