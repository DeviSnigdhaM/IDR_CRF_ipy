{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Image Is Worth a Thousand Words\n",
    "## Build Your First Intelligent Document Recognizer\n",
    "\n",
    "\n",
    "#### Named Entity Recognition(NER) using Conditional Random Fields(CRF)\n",
    "\n",
    "NER is the method of extracting relevant information from a corpus of data and classifying those entities into predefined categories such as merchant, date, amount etc. Conditional Random fields are a type of discriminative machine learning classifiers that model the decision boundary and classify the OCR data into different output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "We will be using a publicly available dataset consisting of receipts. Open a file in the below folders to look at the structure of the OCR outputs and annotations for the dataset we're using for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "allOCRSet = \"datasets/CleanOCR-all\" # A folder with all OCR files\n",
    "allAnnotationSet = \"datasets/Annotations-all\" # A folder with all Annotation files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Optical Character Recognition (OCR)\n",
    "Use Pytesseract to apply OCR on a sample receipt image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see the receipt image \n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='datasets/SplitYB.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Receipt image from a restaurant\n",
    "image_path = \"datasets/SplitYB.png\"\n",
    "\n",
    "# Send receipt image through tesseract OCR\n",
    "def getOCRText(image_path):\n",
    "    image_data = Image.open(image_path)\n",
    "    image_text = pytesseract.image_to_string(image_data)\n",
    "    return image_text\n",
    "\n",
    "print(getOCRText(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Labeling - Tokenize, Annotate and Tag\n",
    "\n",
    "In order to get the text labeled and ready for the CRF model, we have to follow the following three steps:\n",
    "\n",
    "1. <p>Tokenize</p>\n",
    "Break the text into linguistic units. Ex: Tokenize on whitepspaces\n",
    "2. <p>Annotate</p>\n",
    "Label the truth data for the fields to be extracted. ex: date, amount and merchant\n",
    "3. <p>Tag</p> \n",
    "    The IOB (short for inside, outside, beginning) is a common tagging format for tagging tokens.\n",
    "    I- prefix before a tag indicates that the tag is inside a chunk.\n",
    "    B- prefix before a tag indicates that the tag is the beginning of a chunk.\n",
    "    An O tag indicates that a token belongs to no chunk (outside)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allNerDataset = \"datasets/ner-dataset.csv\"\n",
    "\n",
    "def dataAnnotation(OCRset, annotationSet, nerDataset):\n",
    "\tallCleanOCRFiles = os.listdir(OCRset) # Load all Clean OCR Files\n",
    "\tallIXFiles = os.listdir(annotationSet) # Load all Clean OCR Files\n",
    "\toutfile = open(nerDataset, 'w')\n",
    "\treceiptNum = 0\n",
    "\toutfile.write('Receipt #,Word,POS,Tag' + '\\n')\n",
    "\n",
    "\tfor ixfile in allIXFiles:\n",
    "\t\tfilename = os.fsdecode(ixfile)\n",
    "\t\tfor ocrfile in allCleanOCRFiles:\n",
    "\t\t\tocrFilename = os.fsdecode(ocrfile)\n",
    "\t\t\tif filename == ocrFilename and not filename.startswith('.'):\n",
    "\t\t\t\tixData = {}\n",
    "\t\t\t\tocrCleanLine = \"\"\n",
    "\t\t\t\twith open(annotationSet + '/' + filename) as f1:\n",
    "\t\t\t\t\tixData = json.load(f1)\n",
    "\t\t\t\twith open(OCRset + '/' + filename) as f2:\n",
    "\t\t\t\t\tocrCleanLine = f2.readlines()\n",
    "\t\t\t\tannotationLine = \"\"\n",
    "\t\t\t\twordNum = 0\n",
    "\t\t\t\twords = ocrCleanLine[0].replace(',', '').split(' ')\n",
    "\t\t\t\tisAmount = False\n",
    "\t\t\t\tisDate = False\n",
    "\t\t\t\tisMerchant = False\n",
    "\t\t\t\tbeginMerchant = False\n",
    "\t\t\t\tbeginDate = False\n",
    "\n",
    "\t\t\t\tmerchantParts = ixData['company'].split(' ')\n",
    "\t\t\t\tdateParts = ixData['date'].split(' ')\n",
    "\t\t\t\t\n",
    "\t\t\t\tfor word in words:\n",
    "\t\t\t\t\tannotationLine = \"\"\n",
    "\t\t\t\t\tword = word.replace(',', ' ')\n",
    "\t\t\t\t\tif wordNum == 0:\n",
    "\t\t\t\t\t\tif word == dateParts[0] and not isDate:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"DT\" + ',' + 'B-date'\n",
    "\t\t\t\t\t\t\tisDate = True\n",
    "\t\t\t\t\t\t\t# Only expect a I-date if there are multiple parts to the date\n",
    "\t\t\t\t\t\t\tif len(dateParts) > 1:\n",
    "\t\t\t\t\t\t\t\tbeginDate = True\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tbeginDate = False\n",
    "\t\t\t\t\t\telif word == ixData['total'] and not isAmount:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"AM\" + ',' + 'B-amt'\n",
    "\t\t\t\t\t\t\tisAmount = True\n",
    "\t\t\t\t\t\telif word == merchantParts[0] and not isMerchant:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"MR\" + ',' + 'B-mer'\n",
    "\t\t\t\t\t\t\tisMerchant = True\n",
    "\t\t\t\t\t\t\tbeginMerchant = True\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tannotationLine = 'Receipt: ' + str(receiptNum) + ',' + word + ',' + \"NN\" + ',' + 'O'\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t#Date tagging\n",
    "\t\t\t\t\t\tif word == dateParts[0] and not isDate:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"DT\" + ',' + 'B-date'\n",
    "\t\t\t\t\t\t\tisDate = True\n",
    "\t\t\t\t\t\t\t# Only expect a I-date if there are multiple parts to the date\n",
    "\t\t\t\t\t\t\tif len(dateParts) > 1:\n",
    "\t\t\t\t\t\t\t\t# print(\"This date has many parts: \" + ixData['date'])\n",
    "\t\t\t\t\t\t\t\tbeginDate = True\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tbeginDate = False\n",
    "\t\t\t\t\t\telif word in ixData['date'] and word != dateParts[0] and word != dateParts[-1] and beginDate:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"DT\" + ',' + 'I-date'\n",
    "\t\t\t\t\t\telif word == dateParts[-1] and beginDate:\n",
    "\t\t\t\t\t\t\tbeginDate = False\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"DT\" + ',' + 'I-date'\n",
    "\t\t\t\t\t\t# Amount tagging\n",
    "\t\t\t\t\t\telif word == ixData['total'] and not isAmount:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"AM\" + ',' + 'B-amt'\n",
    "\t\t\t\t\t\t\tisAmount = True\n",
    "\n",
    "\t\t\t\t\t\t# Merchant tagging\n",
    "\t\t\t\t\t\telif word == merchantParts[0] and not isMerchant:\n",
    "\t\t\t\t\t\t\tbeginMerchant = True\n",
    "\t\t\t\t\t\t\tisMerchant = True\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"MR\" + ',' + 'B-mer'\n",
    "\t\t\t\t\t\telif word in ixData['company'] and word != merchantParts[0] and word != merchantParts[-1] and beginMerchant:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"MR\" + ',' + 'I-mer'\n",
    "\t\t\t\t\t\telif word == merchantParts[-1] and beginMerchant:\n",
    "\t\t\t\t\t\t\tbeginMerchant = False\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"MR\" + ',' + 'I-mer'\n",
    "\n",
    "\t\t\t\t\t\t#Other tagging\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tannotationLine = ',' + word + ',' + \"NN\" + ',' + 'O'\n",
    "\t\t\t\t\twordNum += 1\t\t\t\t\t\t\n",
    "\t\t\t\t\toutfile.write(annotationLine + '\\n')\n",
    "\t\t\t\treceiptNum += 1\n",
    "\toutfile.close\n",
    "\tprint ('Annotations for %d files completed!' %receiptNum)\n",
    "dataAnnotation(allOCRSet, allAnnotationSet, allNerDataset)"
   ]
  },
  {
   "attachments": {
    "CodeCogsEqn1-6.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Model Training\n",
    "\n",
    "CRF is used for predicting sequences using contextual information. The CRF model is a supervised model that requires labelled data. The data required for our model training consists of tokens from a particular receipt with their corresponding Part of Speech (POS) tag and their BIO scheme tag. \n",
    "\n",
    "Below is the formula for CRF where y is the output variable and X is input sequence:\n",
    "![CodeCogsEqn1-6.png](attachment:CodeCogsEqn1-6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_f1_score\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the csv file\n",
    "df = pd.read_csv('datasets/ner-dataset-1000.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "#Display first 10 rows\n",
    "df.head(10)\n",
    "df.describe()\n",
    "#Displaying the unique Tags\n",
    "df['Tag'].unique()\n",
    "#Checking null values, if any.\n",
    "df.isnull().sum()\n",
    "df = df.fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentence(object):\n",
    "    def __init__(self, df):\n",
    "        self.n_sent = 1\n",
    "        self.df = df\n",
    "        self.empty = False\n",
    "        agg = lambda s : [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(),\n",
    "                                                       s['POS'].values.tolist(),\n",
    "                                                       s['Tag'].values.tolist())]\n",
    "        self.grouped = self.df.groupby(\"Receipt #\").apply(agg)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    def get_text(self):\n",
    "        try:\n",
    "            s = self.grouped['Receipt: {}'.format(self.n_sent)]\n",
    "            self.n_sent +=1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "#Displaying one full sentence\n",
    "getter = sentence(df)\n",
    "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
    "sentences[0]\n",
    "\n",
    "#sentence with its pos and tag.\n",
    "sent = getter.get_text()\n",
    "# print(sent)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Functions\n",
    "\n",
    "We will be using the default features used by the NER in Natural Language Toolkit (NLTK) as a starting point. This includes word identity, word suffix, word shape and word POS tag in addition to some information from nearby words. We can modify and add to these features in order to improve accuracy as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele   \n",
    "    \n",
    "    # return string   \n",
    "    return str1\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The data is then fed into the CRF model trainer provided by Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.6)\n",
    "\n",
    "crf = CRF(algorithm = 'lbfgs',\n",
    "         c1 = 0.1,\n",
    "         c2 = 0.1,\n",
    "         max_iterations = 100,\n",
    "         all_possible_transitions = False)\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting on the test set.\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "def printPredictions(y_pred):\n",
    "\tfor i in range(len(y_pred)):\n",
    "\t\tprediction = y_pred[i]\n",
    "\t\ttestList = X_test[i]\n",
    "\t\ttestSentence = \"\"\n",
    "\t\tfor testTuple in testList:\n",
    "\t\t\ttestSentence = testSentence + testTuple['word.lower()'] + ' '\n",
    "\t\twords = testSentence.split(\" \")\n",
    "\t\tx = 0\n",
    "\t\tfor wordPrediction in prediction:\n",
    "\t\t\tif wordPrediction == 'B-date' or wordPrediction == 'B-amt' or wordPrediction == 'B-mer' or wordPrediction == 'I-mer' or wordPrediction == 'I-date':\n",
    "\t\t\t\tprint('%s : %s' % (wordPrediction, words[x]))\n",
    "\t\t\tx +=1\n",
    "\n",
    "printPredictions(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Report and Analyze Results\n",
    "Print the results of how your model was trained and tested to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = flat_f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"Flat F1 Score: %f \\n\" %f1_score)\n",
    "\n",
    "report = flat_classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application\n",
    "\n",
    "Now, lets test a new receipt that the machine learning model hasn't encountered before.\n",
    "\n",
    "- Option 1: Upload a receipt image to the dataset directory.\n",
    "- Option 2: Use the lunch.png image available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test End to End by uploading a new image - OCR to NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see the receipt image \n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='datasets/lunch.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "new_image_path = \"datasets/lunch.png\"\n",
    "test_file = getOCRText(new_image_path)\n",
    "\n",
    "test_ocr_sentence = test_file.replace(\"\\n\", \" \")\n",
    "print(test_ocr_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the test image\n",
    "\n",
    "1. Add the OCR output from above to the OCR file in CleanOCR-test directory\n",
    "2. Add the corresponding annotations to a file in the Annotations-test directory\n",
    "\n",
    "Ensure that the 2 files have identical names in order for the next step to work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testOCRSet = \"datasets/CleanOCR-test\" # A folder with all OCR files\n",
    "testAnnotationSet = \"datasets/Annotations-test\" # A folder with all Annotation files\n",
    "testNerDataset = \"datasets/ner-dataset-test.csv\"\n",
    "\n",
    "dataAnnotation(testOCRSet,testAnnotationSet, testNerDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_sentence(test_file):\n",
    "    #Reading the csv file\n",
    "    trial = pd.read_csv(test_file, encoding = \"ISO-8859-1\")\n",
    "\n",
    "    #Display first 10 rows\n",
    "    trial.head(10)\n",
    "    trial.describe()\n",
    "    #Displaying the unique Tags\n",
    "    trial['Tag'].unique()\n",
    "    #Checking null values, if any.\n",
    "    trial.isnull().sum()\n",
    "    trial = trial.fillna(method = 'ffill')\n",
    "    trial_getter = sentence(trial) # Check last token - seems to be repeating \n",
    "    trial_sentences = [\" \".join([s[0] for s in sent]) for sent in trial_getter.sentences]\n",
    "    trial_sentences[0]\n",
    "\n",
    "    #sentence with its pos and tag.\n",
    "    trial_sent = trial_getter.get_text()\n",
    "    trial_sentences = trial_getter.sentences\n",
    "    return trial_sentences\n",
    "\n",
    "trial_sentences = get_test_sentence(testNerDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = [sent2features(s) for s in trial_sentences]\n",
    "ty = [sent2labels(s) for s in trial_sentences]\n",
    "\n",
    "#Predicting on the one test set.\n",
    "X_test = tX\n",
    "y_test = ty\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "f1_score = flat_f1_score(y_test, y_pred, average = 'weighted')\n",
    "print(\"Flat F1 Score: %f \\n\" %f1_score)\n",
    "\n",
    "report = flat_classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "printPredictions(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
